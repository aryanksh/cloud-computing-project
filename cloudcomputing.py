# -*- coding: utf-8 -*-
"""CloudComputing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13RdYBsqq42cImqIwknvFbgPKH-zB73e0
"""

!pip install shap lime

import pandas as pd
import numpy as np
from itertools import combinations
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from collections import Counter
from sklearn.preprocessing import MultiLabelBinarizer

households = pd.read_csv('400_households.csv')

households.columns = households.columns.str.strip()

households.info()

households.head()

products = pd.read_csv('400_products.csv')

products.columns = products.columns.str.strip()

products.info()

transactions = pd.read_csv('400_transactions.csv')

transactions.columns = transactions.columns.str.strip()

transactions.info()

transactions['PURCHASE_'] = pd.to_datetime(transactions['PURCHASE_'], errors='coerce')

# Step 2: Merge transactions with households
transactions_households = transactions.merge(households, on='HSHD_NUM', how='left')

# Step 3: Merge the result with products
full_data = transactions_households.merge(products, on='PRODUCT_NUM', how='left')

full_data['PURCHASE_'] = pd.to_datetime(full_data['PURCHASE_'], errors='coerce')

# Dataset reference end date
dataset_end_date = pd.to_datetime('2020-08-15')

# Define date windows
last_30_days = dataset_end_date - pd.Timedelta(days=30)
days_31_to_90 = dataset_end_date - pd.Timedelta(days=90)

# Recent 30-day spend and transactions
recent_30 = full_data[(full_data['PURCHASE_'] >= last_30_days) & (full_data['PURCHASE_'] <= dataset_end_date)]
recent_agg = recent_30.groupby('HSHD_NUM').agg(
    spend_recent_30=('SPEND', 'sum'),
    transactions_recent_30=('BASKET_NUM', 'nunique')
).reset_index()

# 31–90 day spend and transactions
mid_90 = full_data[(full_data['PURCHASE_'] >= days_31_to_90) & (full_data['PURCHASE_'] < last_30_days)]
mid_agg = mid_90.groupby('HSHD_NUM').agg(
    spend_mid_90=('SPEND', 'sum'),
    transactions_mid_90=('BASKET_NUM', 'nunique')
).reset_index()

# Lifetime aggregates
lifetime_agg = full_data.groupby('HSHD_NUM').agg(
    spend_lifetime=('SPEND', 'sum'),
    transactions_lifetime=('BASKET_NUM', 'nunique'),
    first_purchase=('PURCHASE_', 'min'),
    last_purchase=('PURCHASE_', 'max')
).reset_index()

# Merge all
behavior = lifetime_agg.merge(recent_agg, on='HSHD_NUM', how='left')
behavior = behavior.merge(mid_agg, on='HSHD_NUM', how='left')

# Fill missing values
behavior = behavior.fillna(0)

# Days active
behavior['days_active'] = (behavior['last_purchase'] - behavior['first_purchase']).dt.days

# Recent engagement ratios
behavior['spend_recent_ratio'] = behavior['spend_recent_30'] / (behavior['spend_mid_90'] + 1)
behavior['transactions_recent_ratio'] = behavior['transactions_recent_30'] / (behavior['transactions_mid_90'] + 1)

# Drop rates
behavior['spend_drop_pct'] = (behavior['spend_mid_90'] - behavior['spend_recent_30']) / (behavior['spend_mid_90'] + 1)
behavior['transactions_drop_pct'] = (behavior['transactions_mid_90'] - behavior['transactions_recent_30']) / (behavior['transactions_mid_90'] + 1)

# Recent monetary per transaction
behavior['avg_spend_per_transaction_recent'] = behavior['spend_recent_30'] / (behavior['transactions_recent_30'] + 1)

# Churn = major drop (> 80% spend drop) OR no recent activity
behavior['churn'] = np.where(
    (behavior['spend_drop_pct'] > 0.8) | (behavior['transactions_recent_30'] == 0),
    1,
    0
)

behavior.info()
behavior.head()

# Let's train and compare a bunch of classifier models on the smarter churn data using LIME and Partial Dependence!

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Optional: LIME for explanation
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt

# 1. Feature Selection
feature_cols = [
    'spend_recent_30', 'spend_mid_90', 'spend_recent_ratio', 'spend_drop_pct',
    'transactions_recent_30', 'transactions_mid_90', 'transactions_recent_ratio',
    'transactions_drop_pct', 'avg_spend_per_transaction_recent'
]
X = behavior[feature_cols].values
y = behavior['churn'].values

# 2. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=42, stratify=y
)

# 3. Standardize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Define Models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'Support Vector Machine': SVC(probability=True, random_state=42)
}

best_model = None
best_accuracy = 0

# 5. Train and Evaluate
for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    print(f"Accuracy for {name}: {acc:.4f}")
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    if acc > best_accuracy:
        best_accuracy = acc
        best_model = model

print(f"\nBest model: {best_model.__class__.__name__} with accuracy {best_accuracy:.4f}")

# 6. Explanation with LIME
print("\nExplaining a test instance with LIME...")

# Create a LIME explainer
explainer = LimeTabularExplainer(
    training_data=X_train_scaled,
    feature_names=feature_cols,
    class_names=['active', 'churn'],
    mode='classification'
)

# Explain the first test sample
i = 2
exp = explainer.explain_instance(
    X_test_scaled[i],
    best_model.predict_proba,
    num_features=5
)
plt = exp.as_pyplot_figure()
plt.tight_layout()
# Display explanation
exp.show_in_notebook(show_table=True)

# 7. Alternative: Partial Dependence Plots (sklearn >=0.24)
from sklearn.inspection import PartialDependenceDisplay

print("\nGenerating Partial Dependence for key features...")
# Display PDP for 'spend_recent_30' (idx 0) and 'spend_drop_pct' (idx 3)
PartialDependenceDisplay.from_estimator(
    best_model,
    X_train_scaled,
    features=[0, 3],  # indices of features in feature_cols
    feature_names=feature_cols
)
plt.show()

feat_i = feature_cols.index('spend_recent_ratio')
z = -0.13
real_cutoff = z * scaler.scale_[feat_i] + scaler.mean_[feat_i]
print(f"Raw spend_recent_ratio cutoff ≈ {real_cutoff:.2f}")

"""basket analysis"""

product_to_commodity = products.set_index('PRODUCT_NUM')['COMMODITY'].to_dict()
transactions['COMMODITY'] = transactions['PRODUCT_NUM'].map(product_to_commodity)

# Step 2: Group baskets by BASKET_NUM and collect all COMMODITYs
basket_categories = transactions.groupby('BASKET_NUM')['COMMODITY'].apply(list).reset_index()

# Sample a smaller number of baskets to avoid memory issues
sampled_baskets_small = transactions['BASKET_NUM'].drop_duplicates().sample(n=2000, random_state=42)
sampled_transactions_small = transactions[transactions['BASKET_NUM'].isin(sampled_baskets_small)]

# Map product numbers to commodity names again
sampled_transactions_small['COMMODITY'] = sampled_transactions_small['PRODUCT_NUM'].map(product_to_commodity)

# Group by BASKET_NUM to get list of commodity categories per basket
basket_categories_small = sampled_transactions_small.groupby('BASKET_NUM')['COMMODITY'].apply(list).reset_index()

# Display a preview
basket_categories_small.head()

# Flatten the list of all commodities in the sample and count frequencies
from collections import Counter

all_categories = [cat for sublist in basket_categories_small['COMMODITY'] for cat in sublist]
category_counter = Counter(all_categories)

# Show the 10 most common categories
category_counts_small = pd.DataFrame(category_counter.most_common(10), columns=['COMMODITY', 'COUNT'])
category_counts_small

# Step 1: Extract the unique categories
unique_categories = sorted(list(category_counter.keys()))

# Step 2: Prepare the one-hot encoded matrix for all baskets
mlb = MultiLabelBinarizer(classes=unique_categories)
X_full = mlb.fit_transform(basket_categories_small['COMMODITY'])
df_basket_ohe = pd.DataFrame(X_full, columns=mlb.classes_)

# Strip trailing spaces from column names for easier reference
df_basket_ohe.columns = df_basket_ohe.columns.str.strip()
cleaned_unique_categories = df_basket_ohe.columns.tolist()

# Step 3: For each category, train a Random Forest model to predict its presence
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

category_importances = {}

for category in cleaned_unique_categories:
    # Prepare target: 1 if category present, 0 otherwise
    y = df_basket_ohe[category].values
    X = df_basket_ohe.drop(columns=[category]).values  # Drop target from input

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Store feature importances for this target category
    importances = model.feature_importances_
    predictors = df_basket_ohe.drop(columns=[category]).columns
    category_importances[category] = dict(zip(predictors, importances))


# Convert to DataFrame for one category (e.g., DAIRY) as an example
importances_dairy = pd.DataFrame(
    sorted(category_importances['DAIRY'].items(), key=lambda x: x[1], reverse=True),
    columns=['Predictor_Category', 'Importance']
)

importances_dairy

importances_international= pd.DataFrame(
    sorted(category_importances['INTERNATIONAL FOOD'].items(), key=lambda x: x[1], reverse=True),
    columns=['Predictor_Category', 'Importance']
)

importances_international

products.head(1)

